{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4rNQ-LMklgn",
        "outputId": "b79a9dec-cc6c-4618-cc78-3235865f761d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.13.1)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505177 sha256=f6c36f3a5e210f8e1d97ec061efaf90a70bdd7d8e72b5d872b4c5145aa540fb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfSEb4xQkNyl",
        "outputId": "71f955a0-1e98-432a-e284-b5bf2cb406cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Post Interactions (Top 5 Rows):\n",
            "   user_id  post_id  interaction\n",
            "0        1      363            3\n",
            "1        1       74            1\n",
            "2        1      376            1\n",
            "3        1      156            1\n",
            "4        1      105            1\n",
            "User-Post Interactions (Top 5 tails):\n",
            "      user_id  post_id  interaction\n",
            "1302      100       86            2\n",
            "1303      100      108            0\n",
            "1304      100      479            0\n",
            "1305      100       84            2\n",
            "1306      100      298            1\n",
            "\n",
            "Post Data (Top 5 Rows):\n",
            "   post_id                        tags  \\\n",
            "0        1            politics, movies   \n",
            "1        2              movies, health   \n",
            "2        3   movies, politics, fashion   \n",
            "3        4             movies, fashion   \n",
            "4        5  science, fashion, politics   \n",
            "\n",
            "                                             content  \n",
            "0  Exploring the role of women in science and tec...  \n",
            "1  The latest breakthrough in AI is set to change...  \n",
            "2  Understanding the complexities of modern polit...  \n",
            "3  The challenges of modern medicine in the 21st ...  \n",
            "4     Exploring the wonders of deep-sea exploration.  \n",
            "\n",
            "Post Data (Top 5 tails):\n",
            "     post_id                         tags  \\\n",
            "495      496  sports, technology, science   \n",
            "496      497               movies, sports   \n",
            "497      498   sports, movies, technology   \n",
            "498      499              fashion, movies   \n",
            "499      500           technology, sports   \n",
            "\n",
            "                                               content  \n",
            "495  Mindfulness techniques that can reduce stress ...  \n",
            "496  The role of AI in shaping modern healthcare sy...  \n",
            "497        Understanding the basics of cryptocurrency.  \n",
            "498     Top musicians collaborate for a charity event.  \n",
            "499  The role of AI in shaping modern healthcare sy...  \n",
            "Best RMSE: 0.9948993924218023\n",
            "Best Parameters: {'n_factors': 100, 'n_epochs': 70, 'lr_all': 0.0001, 'reg_all': 0.4}\n",
            "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
            "RMSE (testset)    1.0287  0.9661  1.0115  1.0021  0.0264  \n",
            "MAE (testset)     0.8498  0.8072  0.8001  0.8190  0.0220  \n",
            "Fit time          0.06    0.06    0.06    0.06    0.00    \n",
            "Test time         0.00    0.00    0.00    0.00    0.00    \n",
            "CV Results: {'test_rmse': array([1.02866142, 0.96606094, 1.01146722]), 'test_mae': array([0.84981133, 0.80722928, 0.80006598]), 'fit_time': (0.06185579299926758, 0.06298327445983887, 0.0573272705078125), 'test_time': (0.0039365291595458984, 0.003410816192626953, 0.004815340042114258)}\n",
            "Recommended posts for user 1: [125, 232, 379, 442, 220]\n",
            "Recommended posts for user 2: [475, 340, 108, 395, 201]\n",
            "Recommended posts for user 3: [457, 204, 317, 8, 321]\n",
            "Recommended posts for user 4: [271, 358, 406, 329, 343]\n",
            "Recommended posts for user 5: [103, 177, 442, 426, 15]\n",
            "Recommended posts for user 6: [442, 433, 79, 492, 177]\n",
            "Recommended posts for user 7: [420, 68, 353, 389, 32]\n",
            "Recommended posts for user 8: [141, 190, 6, 310, 290]\n",
            "Recommended posts for user 9: [171, 139, 290, 279, 293]\n",
            "Recommended posts for user 10: [206, 101, 350, 153, 433]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from surprise import accuracy\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "num_users = 100\n",
        "num_posts = 500\n",
        "\n",
        "# Generate user and post IDs\n",
        "user_ids = np.arange(1, num_users + 1)\n",
        "post_ids = np.arange(1, num_posts + 1)\n",
        "\n",
        "tags_list = ['politics', 'sports', 'technology', 'movies', 'science', 'health', 'fashion']\n",
        "\n",
        "post_content_samples = [\n",
        "    \"The latest breakthrough in AI is set to change the industry.\",\n",
        "    \"A thrilling sports match ended with an unexpected twist!\",\n",
        "    \"Scientists have discovered a new planet that may support life.\",\n",
        "    \"The upcoming movie is getting fantastic early reviews.\",\n",
        "    \"Health experts discuss the importance of mental well-being.\",\n",
        "    \"A guide to the best travel destinations for food lovers.\",\n",
        "    \"The history behind one of the most famous ancient civilizations.\",\n",
        "    \"New gaming consoles are redefining the entertainment experience.\",\n",
        "    \"An inside look at the fashion trends for the next season.\",\n",
        "    \"Top musicians collaborate for a charity event.\",\n",
        "    \"A revolutionary way to combat climate change has emerged.\",\n",
        "    \"New studies suggest a healthy diet improves productivity.\",\n",
        "    \"The most anticipated movie of the year is finally here.\",\n",
        "    \"Exploring the wonders of deep-sea exploration.\",\n",
        "    \"The role of AI in shaping modern healthcare systems.\",\n",
        "    \"The top 5 gadgets you need for 2025.\",\n",
        "    \"How to travel on a budget without missing out.\",\n",
        "    \"The future of space exploration and human colonization.\",\n",
        "    \"The impact of social media on modern relationships.\",\n",
        "    \"Fashion trends to expect this winter season.\",\n",
        "    \"Understanding the complexities of modern politics.\",\n",
        "    \"How technology is transforming education for the better.\",\n",
        "    \"The latest developments in the tech industry.\",\n",
        "    \"Exploring the intersection of art and technology.\",\n",
        "    \"The top fitness trends to try this year.\",\n",
        "    \"Mindfulness techniques that can reduce stress levels.\",\n",
        "    \"The best food destinations around the world.\",\n",
        "    \"A deep dive into the most famous scientific experiments.\",\n",
        "    \"The influence of historical events on today's society.\",\n",
        "    \"Sports and mental health: A connection worth exploring.\",\n",
        "    \"Future trends in environmental sustainability.\",\n",
        "    \"How virtual reality is revolutionizing gaming.\",\n",
        "    \"Top 10 places to visit before you die.\",\n",
        "    \"The challenges of modern medicine in the 21st century.\",\n",
        "    \"The effects of global warming on wildlife.\",\n",
        "    \"Exploring the role of women in science and technology.\",\n",
        "    \"How to stay active while working from home.\",\n",
        "    \"Understanding the basics of cryptocurrency.\",\n",
        "    \"The importance of sleep in maintaining health.\",\n",
        "    \"How to build a successful online business.\"\n",
        "]\n",
        "\n",
        "# Generate random interactions (likes, comments, shares) between users and posts\n",
        "interactions = []\n",
        "for user_id in user_ids:\n",
        "    for post_id in np.random.choice(post_ids, size=np.random.randint(5, 20), replace=False):  # Each user interacts with 5-20 posts\n",
        "        interaction_value = np.random.choice([0, 1, 2, 3], p=[0.4, 0.3, 0.2, 0.1])  # 40% no interaction, 30% like, 20% comment, 10% share\n",
        "        interactions.append((user_id, post_id, interaction_value))\n",
        "\n",
        "# Create a DataFrame for user-item interactions\n",
        "interaction_data = pd.DataFrame(interactions, columns=[\"user_id\", \"post_id\", \"interaction\"])\n",
        "\n",
        "# Assign random tags and content to posts\n",
        "post_data = pd.DataFrame({\n",
        "    \"post_id\": post_ids,\n",
        "    \"tags\": [', '.join(np.random.choice(tags_list, size=np.random.randint(2, 4), replace=False)) for _ in post_ids],  # 2 to 3 tags per post\n",
        "    \"content\": [random.choice(post_content_samples) for _ in post_ids]  # Randomly assigning content\n",
        "})\n",
        "\n",
        "# Save to CSV files (optional)\n",
        "interaction_data.to_csv(\"user_interactions.csv\", index=False)\n",
        "post_data.to_csv(\"post_tags_content.csv\", index=False)\n",
        "\n",
        "# Display a sample of the data\n",
        "print(\"User-Post Interactions (Top 5 Rows):\")\n",
        "print(interaction_data.head())\n",
        "print(\"User-Post Interactions (Top 5 tails):\")\n",
        "print(interaction_data.tail())\n",
        "\n",
        "print(\"\\nPost Data (Top 5 Rows):\")\n",
        "print(post_data.head())\n",
        "print(\"\\nPost Data (Top 5 tails):\")\n",
        "print(post_data.tail())\n",
        "\n",
        "# Load datasets\n",
        "interaction_data = pd.read_csv(\"user_interactions.csv\")\n",
        "post_data = pd.read_csv(\"post_tags_content.csv\")\n",
        "\n",
        "# Prepare data for Surprise\n",
        "reader = Reader(rating_scale=(0, 3))\n",
        "data = Dataset.load_from_df(interaction_data[['user_id', 'post_id', 'interaction']], reader)\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100, 200],\n",
        "    'n_epochs': [30, 50, 70],\n",
        "    'lr_all': [0.0001, 0.0005, 0.001],\n",
        "    'reg_all': [0.2, 0.3, 0.4]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "gs.fit(data)\n",
        "\n",
        "# Best parameters and model\n",
        "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
        "print(f\"Best Parameters: {gs.best_params['rmse']}\")\n",
        "best_svd = gs.best_estimator['rmse']\n",
        "\n",
        "# Perform cross-validation to evaluate the model with multiple train-test splits\n",
        "cv_results = cross_validate(best_svd, data, measures=['rmse', 'mae'], cv=3, verbose=True)\n",
        "print(f\"CV Results: {cv_results}\")\n",
        "\n",
        "# Fit the best model\n",
        "trainset = data.build_full_trainset()\n",
        "best_svd.fit(trainset)\n",
        "\n",
        "# Advanced content-based filtering using TF-IDF\n",
        "def advanced_content_based_filtering(post_data):\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf.fit_transform(post_data['tags'] + \" \" + post_data['content'])\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "# Precompute cosine similarity matrix\n",
        "content_sim = advanced_content_based_filtering(post_data)\n",
        "\n",
        "# Get content-based recommendations\n",
        "def get_content_based_recommendations(user_interactions, post_data, cosine_sim, num_recommendations=5):\n",
        "    recommended_posts = []\n",
        "    for post_id in user_interactions:\n",
        "        post_idx = post_data[post_data['post_id'] == post_id].index[0]\n",
        "        similarity_scores = list(enumerate(cosine_sim[post_idx]))\n",
        "        sorted_similar_posts = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "        for idx, _ in sorted_similar_posts:\n",
        "            if post_data.iloc[idx]['post_id'] not in user_interactions:\n",
        "                recommended_posts.append(post_data.iloc[idx]['post_id'])\n",
        "                if len(recommended_posts) >= num_recommendations * 2:  # Get more candidates\n",
        "                    break\n",
        "    return recommended_posts\n",
        "\n",
        "# Cold start handling: Recommend popular posts\n",
        "def cold_start_recommendation(post_data, num_recommendations=5):\n",
        "    popular_posts = interaction_data[interaction_data['interaction'] > 0]['post_id'].value_counts().index.tolist()\n",
        "    return popular_posts[:num_recommendations]\n",
        "\n",
        "# Diversify recommendations using Maximal Marginal Relevance (MMR)\n",
        "def diversify_recommendations_mmr(recommendations, post_data, cosine_sim, lambda_param=0.5):\n",
        "    diversified_recommendations = []\n",
        "    remaining_recommendations = recommendations.copy()\n",
        "\n",
        "    while remaining_recommendations:\n",
        "        scores = []\n",
        "        for post_id in remaining_recommendations:\n",
        "            post_idx = post_data[post_data['post_id'] == post_id].index[0]\n",
        "            similarity_to_selected = [cosine_sim[post_idx][post_data[post_data['post_id'] == pid].index[0]] for pid in diversified_recommendations]\n",
        "            max_similarity = max(similarity_to_selected) if similarity_to_selected else 0\n",
        "            scores.append((post_id, max_similarity))\n",
        "\n",
        "        # Select the post with the lowest max similarity\n",
        "        selected_post = min(scores, key=lambda x: x[1])[0]\n",
        "        diversified_recommendations.append(selected_post)\n",
        "        remaining_recommendations.remove(selected_post)\n",
        "\n",
        "    return diversified_recommendations\n",
        "\n",
        "# Hybrid recommendation combining collaborative and content-based filtering\n",
        "def hybrid_recommendation(user_id, num_recommendations=5, collaborative_weight=0.7, content_weight=0.3):\n",
        "    all_post_ids = interaction_data['post_id'].unique()\n",
        "    user_interactions = interaction_data[interaction_data['user_id'] == user_id]['post_id'].tolist()\n",
        "\n",
        "    if not user_interactions:\n",
        "        return cold_start_recommendation(post_data, num_recommendations)\n",
        "\n",
        "    # Adjust weights based on user behavior\n",
        "    if len(user_interactions) < 10:  # Cold start or low interaction\n",
        "        collaborative_weight = 0.5\n",
        "        content_weight = 0.5\n",
        "\n",
        "    # Collaborative filtering recommendations\n",
        "    def predict_ratings(user_id, post_ids, model):\n",
        "        return [(post_id, model.predict(user_id, post_id).est) for post_id in post_ids]\n",
        "\n",
        "    collaborative_ratings = Parallel(n_jobs=-1)(delayed(predict_ratings)(user_id, post_ids, best_svd) for post_ids in np.array_split(all_post_ids, 10))\n",
        "    collaborative_ratings = [item for sublist in collaborative_ratings for item in sublist]\n",
        "    collaborative_ratings = sorted(collaborative_ratings, key=lambda x: x[1], reverse=True)\n",
        "    collaborative_recommendations = [post_id for post_id, _ in collaborative_ratings[:num_recommendations * 2]]  # Get more candidates\n",
        "\n",
        "    # Content-based filtering recommendations\n",
        "    content_recommendations = get_content_based_recommendations(user_interactions, post_data, content_sim, num_recommendations * 2)  # Get more candidates\n",
        "\n",
        "    # Combine recommendations with weights\n",
        "    all_recommendations = list(set(collaborative_recommendations + content_recommendations))\n",
        "\n",
        "    # Rank combined recommendations by weighted score\n",
        "    ranked_recommendations = []\n",
        "    for post_id in all_recommendations:\n",
        "        collaborative_score = next((score for pid, score in collaborative_ratings if pid == post_id), 0)\n",
        "        content_score = max([content_sim[post_data[post_data['post_id'] == post_id].index[0]][post_data[post_data['post_id'] == pid].index[0]] for pid in user_interactions], default=0)\n",
        "        weighted_score = (collaborative_weight * collaborative_score) + (content_weight * content_score)\n",
        "        ranked_recommendations.append((post_id, weighted_score))\n",
        "\n",
        "    ranked_recommendations = sorted(ranked_recommendations, key=lambda x: x[1], reverse=True)\n",
        "    final_recommendations = [post_id for post_id, _ in ranked_recommendations[:num_recommendations * 2]]  # Get more candidates\n",
        "\n",
        "    # Diversify recommendations\n",
        "    diversified_recommendations = diversify_recommendations_mmr(final_recommendations, post_data, content_sim)\n",
        "    return diversified_recommendations[:num_recommendations]\n",
        "\n",
        "# Generate recommendations for sample users\n",
        "for user in range(1, 11):\n",
        "    recommendations = hybrid_recommendation(user)\n",
        "    print(f\"Recommended posts for user {user}: {recommendations}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5u-GHTBokkCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}